{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ IMAGE & CONVERT INTO GRAYSCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(img):\n",
    "    img = img[70:195,78:172]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(img, (50, 50))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(val, val_len, folder, bar_size=20):\n",
    "    progr = \"#\"*round((val)*bar_size/val_len) + \" \"*round((val_len - (val))*bar_size/val_len)\n",
    "    if val == 0:\n",
    "        print(\"\", end = \"\\n\")\n",
    "    else:\n",
    "        print(\"[%s] (%d samples)\\t label : %s \\t\\t\" % (progr, val+1, folder), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[####################] (77 samples)\t label : Ariel_Sharon \t\t\n",
      "[####################] (60 samples)\t label : Junichiro_Koizumi \t\t\n",
      "[####################] (84 samples)\t label : Phillia \t\t\r"
     ]
    }
   ],
   "source": [
    "dataset_folder = \"dataset/\"\n",
    "\n",
    "names = []\n",
    "images = []\n",
    "for folder in os.listdir(dataset_folder):\n",
    "    files = os.listdir(os.path.join(dataset_folder, folder))[:150]\n",
    "    if len(files) < 50 :\n",
    "        continue\n",
    "    for i, name in enumerate(files): \n",
    "        if name.find(\".jpg\") > -1 :\n",
    "            img = cv2.imread(os.path.join(dataset_folder + folder, name))\n",
    "            img = detect_face(img) # detect face using mtcnn and crop to 100x100\n",
    "            if img is not None :\n",
    "                images.append(img)\n",
    "                names.append(folder)\n",
    "\n",
    "                print_progress(i, len(files), folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples : 221\n"
     ]
    }
   ],
   "source": [
    "print(\"number of samples :\", len(names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENCODING LABEL & CATEGORICALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(names)\n",
    "\n",
    "labels = le.classes_\n",
    "\n",
    "name_vec = le.transform(names)\n",
    "\n",
    "categorical_name_vec = to_categorical(name_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of class : 3\n",
      "['Ariel_Sharon' 'Junichiro_Koizumi' 'Phillia']\n"
     ]
    }
   ],
   "source": [
    "print(\"number of class :\", len(labels))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(name_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(categorical_name_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD MODEL & IMPLEMENT FACE RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_ped(img, label, x0, y0, xt, yt, color=(255,127,0), text_color=(255,255,255)):\n",
    "\n",
    "    (w, h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "    cv2.rectangle(img,\n",
    "                  (x0, y0 + baseline),  \n",
    "                  (max(xt, x0 + w), yt), \n",
    "                  color, \n",
    "                  2)\n",
    "    cv2.rectangle(img,\n",
    "                  (x0, y0 - h),  \n",
    "                  (x0 + w, y0 + baseline), \n",
    "                  color, \n",
    "                  -1)  \n",
    "    cv2.putText(img, \n",
    "                label, \n",
    "                (x0, y0),                   \n",
    "                cv2.FONT_HERSHEY_SIMPLEX,     \n",
    "                0.5,                          \n",
    "                text_color,                \n",
    "                1,\n",
    "                cv2.LINE_AA) \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] finish load model...\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\W10\\AppData\\Local\\Temp\\ipykernel_1344\\486231864.py:24: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  label_text = \"%s (%.2f %%)\" % (labels[idx], confidence)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "# --------- load Haar Cascade model -------------\n",
    "face_cascade = cv2.CascadeClassifier('D:\\Project_Face_Recognition 2\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "# --------- load Keras CNN model -------------\n",
    "model = load_model(\"D:\\Project_Face_Recognition 2\\model-philli.h5\")\n",
    "print(\"[INFO] finish load model...\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened() :\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "        for (x, y, w, h) in faces:\n",
    "            \n",
    "            face_img = gray[y:y+h, x:x+w]\n",
    "            face_img = cv2.resize(face_img, (50, 50))\n",
    "            face_img = face_img.reshape(1, 50, 50, 1)\n",
    "            \n",
    "            result = model.predict(face_img)\n",
    "            idx = result.argmax(axis=1)\n",
    "            confidence = result.max(axis=1)*100\n",
    "            if confidence > 80:\n",
    "                label_text = \"%s (%.2f %%)\" % (labels[idx], confidence)\n",
    "            else :\n",
    "                label_text = \"N/A\"\n",
    "            frame = draw_ped(frame, label_text, x, y, x + w, y + h, color=(0,255,255), text_color=(50,50,50))\n",
    "       \n",
    "        cv2.imshow('Detect Face', frame)\n",
    "    else :\n",
    "        break\n",
    "    if cv2.waitKey(10) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING FILE PICTURE ONE BY ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] finish load model...\n",
      "1/1 [==============================] - 0s 131ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\W10\\AppData\\Local\\Temp\\ipykernel_1344\\136357329.py:41: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  label_text = \"%s (%.2f %%)\" % (labels[idx], confidence)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "\n",
    "# load Haar Cascade model\n",
    "face_cascade = cv2.CascadeClassifier('D:\\\\Project_Face_Recognition 2\\\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "# load Keras CNN model\n",
    "model = load_model(\"D:\\\\Project_Face_Recognition 2\\\\model-philli.h5\")\n",
    "print(\"[INFO] finish load model...\")\n",
    "\n",
    "# Function to draw text and bounding box around detected face\n",
    "def draw_ped(frame, text, x, y, x_plus_w, y_plus_h, color, text_color):\n",
    "    cv2.rectangle(frame, (x, y), (x_plus_w, y_plus_h), color, 2)\n",
    "    cv2.putText(frame, text, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)\n",
    "    return frame\n",
    "\n",
    "# Load image for testing\n",
    "image_path = \"D:\\\\Project_Face_Recognition 2\\\\Raffi-Ahmad-2020.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if image is loaded successfully\n",
    "if image is None:\n",
    "    print(\"Error: Image not loaded.\")\n",
    "else:\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "\n",
    "    # Process each detected face\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = gray[y:y+h, x:x+w]\n",
    "        face_img = cv2.resize(face_img, (50, 50))\n",
    "        face_img = face_img.reshape(1, 50, 50, 1)\n",
    "\n",
    "        # Predict using the loaded model\n",
    "        result = model.predict(face_img)\n",
    "        idx = result.argmax(axis=1)\n",
    "        confidence = result.max(axis=1) * 100\n",
    "        if confidence > 80:\n",
    "            label_text = \"%s (%.2f %%)\" % (labels[idx], confidence)\n",
    "        else:\n",
    "            label_text = \"N/A\"\n",
    "        image = draw_ped(image, label_text, x, y, x + w, y + h, color=(0, 255, 255), text_color=(50, 50, 50))\n",
    "\n",
    "    # Display the image with detected faces\n",
    "    cv2.imshow('Detect Face', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING PICTURE ALL OF THEM BY VISUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# from keras.models import load_model\n",
    "# import os\n",
    "\n",
    "# # Load Haar Cascade model\n",
    "# face_cascade = cv2.CascadeClassifier('D:\\\\Project_Face_Recognition\\\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "# # Load Keras CNN model\n",
    "# model = load_model(\"D:\\\\Project_Face_Recognition 2\\\\model-philli.h5\")\n",
    "# print(\"[INFO] finish load model...\")\n",
    "\n",
    "# # Function to draw text and bounding box around detected face\n",
    "# def draw_ped(frame, text, x, y, x_plus_w, y_plus_h, color, text_color):\n",
    "#     cv2.rectangle(frame, (x, y), (x_plus_w, y_plus_h), color, 2)\n",
    "#     cv2.putText(frame, text, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)\n",
    "#     return frame\n",
    "\n",
    "# # Folder path containing images\n",
    "# folder_path = \"D:\\\\Project_Face_Recognition 2\\\\dataset\\\\Junichiro_Koizumi\"\n",
    "\n",
    "# # Iterate over all files in the folder\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check if file is an image file\n",
    "#         # Load image\n",
    "#         image_path = os.path.join(folder_path, filename)\n",
    "#         image = cv2.imread(image_path)\n",
    "\n",
    "#         # Check if image is loaded successfully\n",
    "#         if image is None:\n",
    "#             print(\"Error: Image not loaded.\")\n",
    "#             continue\n",
    "#         else:\n",
    "#             gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#             # Detect faces in the image\n",
    "#             faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "\n",
    "#             # Process each detected face\n",
    "#             for (x, y, w, h) in faces:\n",
    "#                 face_img = gray[y:y+h, x:x+w]\n",
    "#                 face_img = cv2.resize(face_img, (50, 50))\n",
    "#                 face_img = face_img.reshape(1, 50, 50, 1)\n",
    "\n",
    "#                 # Predict using the loaded model\n",
    "#                 result = model.predict(face_img)\n",
    "#                 idx = result.argmax(axis=1)\n",
    "#                 confidence = result.max(axis=1) * 100\n",
    "#                 if confidence > 80:\n",
    "#                     label_text = \"Phillia (%.2f %%)\" % confidence\n",
    "#                 else:\n",
    "#                     label_text = \"Unknown\"\n",
    "#                 image = draw_ped(image, label_text, x, y, x + w, y + h, color=(0, 255, 255), text_color=(50, 50, 50))\n",
    "\n",
    "#             # Display the image with detected faces\n",
    "#             cv2.imshow('Detect Face', image)\n",
    "#             cv2.waitKey(0)\n",
    "\n",
    "# # Close OpenCV windows\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] finish load model...\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "import os\n",
    "\n",
    "# Load Haar Cascade model\n",
    "face_cascade = cv2.CascadeClassifier('D:\\\\Project_Face_Recognition 2\\\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load Keras CNN model\n",
    "model = load_model(\"D:\\\\Project_Face_Recognition 2\\\\model-philli.h5\")\n",
    "print(\"[INFO] finish load model...\")\n",
    "\n",
    "# Function to draw text and bounding box around detected face\n",
    "def draw_ped(frame, text, x, y, x_plus_w, y_plus_h, color, text_color):\n",
    "    cv2.rectangle(frame, (x, y), (x_plus_w, y_plus_h), color, 2)\n",
    "    cv2.putText(frame, text, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)\n",
    "    return frame\n",
    "\n",
    "# Folder path containing images\n",
    "folder_path = \"D:\\\\Project_Face_Recognition 2\\\\dataset\\\\Ariel_Sharon\"\n",
    "folder_name = os.path.basename(folder_path)  # Extracting the folder name\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check if file is an image file\n",
    "        # Load image\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Check if image is loaded successfully\n",
    "        if image is None:\n",
    "            print(\"Error: Image not loaded.\")\n",
    "            continue\n",
    "        else:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect faces in the image\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "\n",
    "            # Process each detected face\n",
    "            for (x, y, w, h) in faces:\n",
    "                face_img = gray[y:y+h, x:x+w]\n",
    "                face_img = cv2.resize(face_img, (50, 50))\n",
    "                face_img = face_img.reshape(1, 50, 50, 1)\n",
    "\n",
    "                # Predict using the loaded model\n",
    "                result = model.predict(face_img)\n",
    "                idx = result.argmax(axis=1)\n",
    "                confidence = result.max(axis=1) * 100\n",
    "                if confidence > 80:\n",
    "                    label_text = \"{} ({:.2f} %)\".format(folder_name, confidence[0])\n",
    "                else:\n",
    "                    label_text = \"Unknown\"\n",
    "                image = draw_ped(image, label_text, x, y, x + w, y + h, color=(0, 255, 255), text_color=(50, 50, 50))\n",
    "\n",
    "            # Display the image with detected faces\n",
    "            cv2.imshow('Detect Face', image)\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "# Close OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING PICTURE ALL OF THEM BY LIST DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] finish load model...\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "File: Ariel_Sharon_0001.jpg - Confidence: 99.99987\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "File: Ariel_Sharon_0002.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "File: Ariel_Sharon_0003.jpg - Confidence: 99.99996\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "File: Ariel_Sharon_0004.jpg - Confidence: 95.75351\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "File: Ariel_Sharon_0004.jpg - Confidence: 98.34436\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "File: Ariel_Sharon_0005.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "File: Ariel_Sharon_0006.jpg - Confidence: 99.986145\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "File: Ariel_Sharon_0007.jpg - Confidence: 74.553154\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "File: Ariel_Sharon_0008.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "File: Ariel_Sharon_0009.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "File: Ariel_Sharon_0009.jpg - Confidence: 99.544106\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "File: Ariel_Sharon_0010.jpg - Confidence: 99.87043\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "File: Ariel_Sharon_0011.jpg - Confidence: 99.99993\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "File: Ariel_Sharon_0012.jpg - Confidence: 97.231766\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "File: Ariel_Sharon_0013.jpg - Confidence: 99.99991\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "File: Ariel_Sharon_0014.jpg - Confidence: 99.99682\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "File: Ariel_Sharon_0015.jpg - Confidence: 97.85027\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "File: Ariel_Sharon_0016.jpg - Confidence: 99.841545\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "File: Ariel_Sharon_0018.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "File: Ariel_Sharon_0019.jpg - Confidence: 99.999794\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "File: Ariel_Sharon_0020.jpg - Confidence: 99.96165\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "File: Ariel_Sharon_0021.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "File: Ariel_Sharon_0022.jpg - Confidence: 99.99947\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "File: Ariel_Sharon_0023.jpg - Confidence: 73.939255\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "File: Ariel_Sharon_0023.jpg - Confidence: 96.35529\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "File: Ariel_Sharon_0024.jpg - Confidence: 99.98534\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "File: Ariel_Sharon_0025.jpg - Confidence: 99.999954\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "File: Ariel_Sharon_0026.jpg - Confidence: 91.75646\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "File: Ariel_Sharon_0027.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "File: Ariel_Sharon_0028.jpg - Confidence: 99.999664\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "File: Ariel_Sharon_0029.jpg - Confidence: 91.97513\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "File: Ariel_Sharon_0030.jpg - Confidence: 99.64354\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "File: Ariel_Sharon_0031.jpg - Confidence: 99.82012\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "File: Ariel_Sharon_0031.jpg - Confidence: 92.594055\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "File: Ariel_Sharon_0032.jpg - Confidence: 99.95457\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "File: Ariel_Sharon_0033.jpg - Confidence: 99.99714\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "File: Ariel_Sharon_0034.jpg - Confidence: 98.985306\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "File: Ariel_Sharon_0034.jpg - Confidence: 99.99981\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "File: Ariel_Sharon_0035.jpg - Confidence: 99.999985\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "File: Ariel_Sharon_0036.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "File: Ariel_Sharon_0037.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "File: Ariel_Sharon_0038.jpg - Confidence: 75.31739\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "File: Ariel_Sharon_0039.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "File: Ariel_Sharon_0040.jpg - Confidence: 99.98641\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "File: Ariel_Sharon_0041.jpg - Confidence: 99.89207\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "File: Ariel_Sharon_0042.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "File: Ariel_Sharon_0043.jpg - Confidence: 99.85008\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "File: Ariel_Sharon_0044.jpg - Confidence: 99.99993\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "File: Ariel_Sharon_0045.jpg - Confidence: 99.99922\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "File: Ariel_Sharon_0046.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "File: Ariel_Sharon_0047.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "File: Ariel_Sharon_0048.jpg - Confidence: 99.998405\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "File: Ariel_Sharon_0049.jpg - Confidence: 99.786575\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "File: Ariel_Sharon_0050.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "File: Ariel_Sharon_0051.jpg - Confidence: 99.99991\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "File: Ariel_Sharon_0052.jpg - Confidence: 99.99998\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "File: Ariel_Sharon_0053.jpg - Confidence: 99.99398\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "File: Ariel_Sharon_0054.jpg - Confidence: 99.99775\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "File: Ariel_Sharon_0055.jpg - Confidence: 99.99998\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "File: Ariel_Sharon_0056.jpg - Confidence: 99.999466\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "File: Ariel_Sharon_0057.jpg - Confidence: 99.987175\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "File: Ariel_Sharon_0058.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "File: Ariel_Sharon_0059.jpg - Confidence: 99.831474\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "File: Ariel_Sharon_0060.jpg - Confidence: 97.67984\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "File: Ariel_Sharon_0061.jpg - Confidence: 99.97197\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "File: Ariel_Sharon_0062.jpg - Confidence: 99.999985\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "File: Ariel_Sharon_0063.jpg - Confidence: 96.150444\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "File: Ariel_Sharon_0063.jpg - Confidence: 96.353165\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "File: Ariel_Sharon_0064.jpg - Confidence: 99.99996\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "File: Ariel_Sharon_0065.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "File: Ariel_Sharon_0066.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "File: Ariel_Sharon_0067.jpg - Confidence: 99.99975\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "File: Ariel_Sharon_0068.jpg - Confidence: 99.980385\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "File: Ariel_Sharon_0069.jpg - Confidence: 68.35401\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "File: Ariel_Sharon_0070.jpg - Confidence: 99.99962\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "File: Ariel_Sharon_0071.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "File: Ariel_Sharon_0072.jpg - Confidence: 100.0\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "File: Ariel_Sharon_0073.jpg - Confidence: 99.999794\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "File: Ariel_Sharon_0074.jpg - Confidence: 99.96755\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "File: Ariel_Sharon_0075.jpg - Confidence: 99.93703\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "File: Ariel_Sharon_0076.jpg - Confidence: 99.98364\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "File: Ariel_Sharon_0077.jpg - Confidence: 97.73196\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "import os\n",
    "\n",
    "# Load Haar Cascade model\n",
    "face_cascade = cv2.CascadeClassifier('D:\\\\Project_Face_Recognition 2\\\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load Keras CNN model\n",
    "model = load_model(\"D:\\\\Project_Face_Recognition 2\\\\model-philli.h5\")\n",
    "print(\"[INFO] finish load model...\")\n",
    "\n",
    "# Function to calculate confidence score for a face\n",
    "def calculate_confidence(face_img):\n",
    "    face_img = cv2.resize(face_img, (50, 50))\n",
    "    face_img = face_img.reshape(1, 50, 50, 1)\n",
    "\n",
    "    # Predict using the loaded model\n",
    "    result = model.predict(face_img)\n",
    "    confidence = result.max(axis=1) * 100\n",
    "    return confidence[0]\n",
    "\n",
    "# Folder path containing images\n",
    "folder_path = \"D:\\\\Project_Face_Recognition 2\\\\dataset\\\\Ariel_Sharon\"\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check if file is an image file\n",
    "        # Load image\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Check if image is loaded successfully\n",
    "        if image is None:\n",
    "            print(\"Error: Image not loaded.\")\n",
    "            continue\n",
    "        else:\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect faces in the image\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "\n",
    "            # Process each detected face\n",
    "            for (x, y, w, h) in faces:\n",
    "                face_img = gray[y:y+h, x:x+w]\n",
    "                confidence = calculate_confidence(face_img)\n",
    "                print(\"File:\", filename, \"- Confidence:\", confidence)\n",
    "\n",
    "# Close OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
